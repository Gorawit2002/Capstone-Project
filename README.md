# 🎓 Capstone Project – Classification with XGBoost and Random Forest

This capstone project applies supervised machine learning techniques to a real-world classification task. Using Python and Jupyter Notebook, the project demonstrates a complete pipeline — from data preprocessing to model evaluation and hyperparameter tuning.

---

## 🔍 Overview

The project tackles a classification problem on tabular data. It includes data cleaning, feature engineering, model training, cross-validation, and result interpretation using both XGBoost and Random Forest algorithms.

---

## 🧠 Key Features

- Exploratory Data Analysis (EDA) with charts and value distributions  
- Feature scaling, label encoding, and missing value imputation  
- Training multiple models: Random Forest, XGBoost  
- Hyperparameter tuning with GridSearchCV  
- Evaluation with accuracy, precision, recall, F1-score, and confusion matrix  
- Final model comparison and result visualization

---

## 🛠️ Tech Stack

- Python 3  
- Jupyter Notebook  
- Libraries: pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost

---


## 🚀 How to Run

1. Install required packages:
```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost

```
2. Install required packages:
```bash
jupyter notebook

```
3. Open CapstoneProject.ipynb and run all cells.

---

## 📚 What I Learned

- How to structure a machine learning pipeline  
- Practical experience comparing model performance  
- Understanding feature importance and model interpretability  
- Tuning parameters effectively using GridSearchCV  

---

## 📄 License

This project is for educational and portfolio demonstration purposes only.

---

## ⚠️ Disclaimer

This project is intended for **educational purposes only**.  
It is **not financial advice** and should **not** be used for live trading in financial markets.
